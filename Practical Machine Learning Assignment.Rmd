---
title: "Practical Machine Learning Assignment"
author: "Vijay Goel"
date: "January 06, 2015"
output: html_document
---
####Executive Summary
This document builds a model to predict type of exercise from an activity tracking dataset. Random Forest performs better than Decision tree, and yields >99% accuracy.

#### Background and Problem
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.

#### Method used for building model
**Initiation - load libraries, set parameters, data download, data upload, data split into training and validation**
```
library(caret)
library(randomForest)
setwd("~/Training/Machine learning")
set.seed(673)

url<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url, destfile = "./trainRaw.csv", method = "curl")
Raw<- read.csv("trainRaw.csv")

inTrain<- createDataPartition(y=Raw$classe, p=0.9, list=FALSE)
trainRaw<- Raw[inTrain,]
valRaw<- Raw[-inTrain,]

trainy<- trainRaw$classe

```

A few choices were evaluated for volume of inTrain dataset. 0.6, 0.7 and  0.8 and 0.9. Out of sample Accuracy of chosen RF model kept increasing with increasing training data volume. This held true across multiple values of seeds. This is also intuitive because after removing columns with too many NAs, the data volume has become low. Hence, 90% of data was used for training, against usual norm of 60-70%. Details of pre-processing and model selection are below. 


**Basic pre-processing - Identify/remove near zero values, remove columns having no logical correlation to output, remove columns with too many NAs**
```
nsv<-  nearZeroVar(trainRaw, saveMetrics=TRUE)
include<- which(nsv$nzv==FALSE) #use nzv column as source of truth

training<- trainRaw[,include]

exclude<- c(1:6) #generic data uncorrelated to output
NALimit<- dim(training)[1]*0.4 #set limit on when to ignore a column if too many NAs : 40%
for  (i in 1:c(dim(training)[2])) {
  s<- (sum(is.na(training[,i])) > NALimit)*1
  if (s==1) {exclude<- c(exclude,i)} else{exclude<-exclude}
}

exclude<- c(exclude, c(dim(training)[2])) #remove output variables from predictors data frame
training<- training[,-exclude] #52 variables left
trainPP<- training #create pre-processed dataset. was useful in keeping code standard when other models were tested.
```
Choices tried for pre-process were 

1. Removal of columns with Near Zero Value. This likely was not necessary when NA removal was so large and explicit. However, we still ran the data set through this, just to be extra sure. Processing time was not a problem. That said, this method did not kick out all the variables with too many NAs. This implies that some of those variables are useful for prediction and carry important information. However, given limited knowledge of dataset, imputation would not be informed enough. Also, given model was giving reasonable accuracy, this information was discarded.

2. Removal of columns with too many NAs (40% was used as cut-off). This cut-off was clear, because there were no border-line cases. Columns with NAs just had too many NAs to impute.

3. Principal Component Analysis: When tried, this reduced number of PCA factors to 20. However, impact on Accuracy was not favorable. It could have helped with model speed, but given that model building was running very fast, PCAs were discarded.

**Build Model**
```
modRF<- randomForest(trainy~., data=trainPP, ntree=20, norm.votes=FALSE)

```
Models tried were Random Forest and Decision Tree. Performance of Random Forest was consistently better. 

randomForest function was used instead of RF in Caret package because Caret was too processing intensive, and took too long to build the model.

Accuracy varied with choice of seed, which suggests that ensemble might improve performance. However, choosing higher share of training dataset for Random Forest model would also be reasonably close (also observed by fitting models). RF was chosen with more data, instead of choosing Ensemble, to keep model faster.

####Error Estimation
Out of sample error was expected to be slightly lower (~99% accuracy), given accuracy of model on testing data was ~100%. Sensitivity and specificity were also expected to be in 99% range.

**Key Results on validation data to compare out of sample error**
```
This code processes validation (out of sample) data, and creates confusion matrices

confusionMatrix(predict(modRF, trainPP), trainy)

valing<- valRaw[,include]
valy<- valing$classe
valing<- valing[,-exclude] #52 variables left
valPP<- valing

```

```{r echo=FALSE}
library(caret)
library(randomForest)
setwd("~/Training/Machine learning")
Raw<- read.csv("trainRaw.csv")

inTrain<- createDataPartition(y=Raw$classe, p=0.9, list=FALSE)
trainRaw<- Raw[inTrain,]
valRaw<- Raw[-inTrain,]
trainy<- trainRaw$classe
nsv<-  nearZeroVar(trainRaw, saveMetrics=TRUE)
include<- which(nsv$nzv==FALSE) #use nzv column as source of truth
training<- trainRaw[,include]
exclude<- c(1:6) #generic data uncorrelated to output
NALimit<- dim(training)[1]*0.4 #set limit on when to ignore a column if too many NAs : 40%
for  (i in 1:c(dim(training)[2])) {
  s<- (sum(is.na(training[,i])) > NALimit)*1
  if (s==1) {exclude<- c(exclude,i)} else{exclude<-exclude}
}

exclude<- c(exclude, c(dim(training)[2])) #remove output varianvel from predictors data frame
training<- training[,-exclude] #52 variables left
trainPP<- training

modRF<- randomForest(trainy~., data=trainPP, ntree=20, norm.votes=FALSE)
print("Confusion Matrix and statistics for training dataset")
confusionMatrix(predict(modRF, trainPP), trainy)
valing<- valRaw[,include]
valy<- valing$classe
valing<- valing[,-exclude] #52 variables left
valPP<- valing

print("Confusion Matrix and statistics for out of sample dataset")
confusionMatrix(predict(modRF, valPP), valy)
```